{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_analysis.ipynb\n",
    "\n",
    "# This notebook demonstrates how to use a fine-tuned BERT model to perform sentiment analysis\n",
    "# on preprocessed disaster-related tweets.\n",
    "\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Sample tweet\n",
    "sample_tweet = \"This hurricane is devastating! I'm scared for everyone's safety.\"\n",
    "\n",
    "# Tokenize the tweet\n",
    "inputs = tokenizer(sample_tweet, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Predict sentiment\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "# Mapping prediction to sentiment label\n",
    "labels = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "sentiment = labels[prediction]\n",
    "print(f\"Sentiment: {sentiment}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
